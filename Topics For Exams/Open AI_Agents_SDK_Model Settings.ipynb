{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80947cc5",
   "metadata": {},
   "source": [
    "## Open AI_Agents_SDK_Model Settings\n",
    "\n",
    "## اے آئی ایجنٹس ایس ڈی کے ماڈل سیٹنگز کھولیں۔"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ad2ca7",
   "metadata": {},
   "source": [
    "Models\n",
    "\n",
    "The Agents SDK comes with out-of-the-box support for OpenAI models in two flavors:\n",
    "\n",
    "* Recommended: the OpenAIResponsesModel, which calls OpenAI APIs using the new Responses API.\n",
    "\n",
    "* The OpenAIChatCompletionsModel, which calls OpenAI APIs using the Chat Completions API.\n",
    "\n",
    "ماڈلز\n",
    "\n",
    "ایجنٹس SDK دو ذائقوں میں اوپن اے آئی ماڈلز کے لیے آؤٹ آف دی باکس سپورٹ کے ساتھ آتا ہے:\n",
    "\n",
    "* تجویز کردہ: OpenAIresponsesModel، جو OpenAI APIs کو نئے Responses API کا استعمال کرتے ہوئے کال کرتا ہے۔\n",
    "\n",
    "* OpenAIChatCompletionsModel، جو Chat Completions API کا استعمال کرتے ہوئے OpenAI APIs کو کال کرتا ہے۔\n",
    "\n",
    "OpenAI models\n",
    "\n",
    "When you don't specify a model when initializing an Agent, the default model will be used. The default is currently gpt-4.1, which offers a strong balance of predictability for agentic workflows and low latency.\n",
    "\n",
    "If you want to switch to other models like gpt-5, follow the steps in the next section.\n",
    "\n",
    "Default OpenAI model\n",
    "\n",
    "If you want to consistently use a specific model for all agents that do not set a custom model, set the OPENAI_DEFAULT_MODEL environment variable before running your agents.\n",
    "\n",
    "اوپن اے آئی ماڈلز\n",
    "\n",
    "جب آپ ایجنٹ کو شروع کرتے وقت ماڈل کی وضاحت نہیں کرتے ہیں، تو ڈیفالٹ ماڈل استعمال کیا جائے گا۔ پہلے سے طے شدہ فی الحال gpt-4.1 ہے، جو ایجنٹی کام کے بہاؤ اور کم تاخیر کے لیے پیشین گوئی کا مضبوط توازن پیش کرتا ہے۔\n",
    "\n",
    "اگر آپ دوسرے ماڈلز جیسے gpt-5 پر سوئچ کرنا چاہتے ہیں تو اگلے سیکشن میں دیے گئے مراحل پر عمل کریں۔\n",
    "\n",
    "ڈیفالٹ اوپن اے آئی ماڈل\n",
    "\n",
    "اگر آپ ان تمام ایجنٹوں کے لیے مستقل طور پر ایک مخصوص ماڈل استعمال کرنا چاہتے ہیں جو اپنی مرضی کے مطابق ماڈل سیٹ نہیں کرتے ہیں، تو اپنے ایجنٹوں کو چلانے سے پہلے OPENAI_DEFAULT_MODEL ماحولیاتی متغیر سیٹ کریں۔"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c13a4f",
   "metadata": {},
   "source": [
    "GPT-5 Models\n",
    "\n",
    "When you use any of GPT-5's reasoning models (gpt-5, gpt-5-mini, or gpt-5-nano) this way, the SDK applies sensible ModelSettings by default. Specifically, it sets both reasoning.effort and verbosity to \"low\". If you want to build these settings yourself, call agents.models.get_default_model_settings(\"gpt-5\").\n",
    "\n",
    "For lower latency or specific requirements, you can choose a different model and settings. To adjust the reasoning effort for the default model, pass your own ModelSettings:\n",
    "\n",
    "GPT-5 ماڈلز\n",
    "\n",
    "جب آپ اس طرح سے GPT-5 کے کسی بھی ریجننگ ماڈل (gpt-5، gpt-5-mini، یا gpt-5-nano) کا استعمال کرتے ہیں، تو SDK بطور ڈیفالٹ سمجھدار ماڈل سیٹنگز کا اطلاق کرتا ہے۔ خاص طور پر، یہ استدلال اور کوشش دونوں کو \"کم\" پر سیٹ کرتا ہے۔ اگر آپ ان ترتیبات کو خود بنانا چاہتے ہیں تو agents.models.get_default_model_settings(\"gpt-5\") پر کال کریں۔\n",
    "\n",
    "کم تاخیر یا مخصوص تقاضوں کے لیے، آپ ایک مختلف ماڈل اور ترتیبات کا انتخاب کر سکتے ہیں۔ پہلے سے طے شدہ ماڈل کے لیے استدلال کی کوشش کو ایڈجسٹ کرنے کے لیے، اپنی خود کی ModelSettings کو پاس کریں:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92986652",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Export OPENAI_DEFAULT_MODEL=gpt-5\n",
    "# Python3 my_awesome_agent.py\n",
    "\n",
    "from openai.types.shared import Reasoning\n",
    "from agents import Agent, ModelSettings\n",
    "\n",
    "my_agent = Agent(\n",
    "    name=\"My Agent\",\n",
    "    instructions=\"You're a helpful agent.\",\n",
    "    model_settings=ModelSettings(reasoning=Reasoning(effort=\"minimal\"), verbosity=\"low\")\n",
    "    # If OPENAI_DEFAULT_MODEL=gpt-5 is set, passing only model_settings works.\n",
    "    # It's also fine to pass a GPT-5 model name explicitly:\n",
    "    # model=\"gpt-5\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6679709",
   "metadata": {},
   "source": [
    "Specifically for lower latency, using either gpt-5-mini or gpt-5-nano model with reasoning.effort=\"minimal\" will often return responses faster than the default settings. However, some built-in tools (such as file search and image generation) in Responses API do not support \"minimal\" reasoning effort, which is why this Agents SDK defaults to \"low\".\n",
    "\n",
    "Non-GPT-5 Models\n",
    "\n",
    "If you pass a non–GPT-5 model name without custom model_settings, the SDK reverts to generic ModelSettings compatible with any model.\n",
    "\n",
    "Non-OpenAI Models\n",
    "\n",
    "You can use most other non-OpenAI models via the LiteLLM integration. First, install the litellm dependency group:\n",
    "\n",
    "خاص طور پر کم تاخیر کے لیے، یا تو gpt-5-mini یا gpt-5-nano ماڈل کو reasoning.effort=\"minimal\" کے ساتھ استعمال کرنے سے اکثر ڈیفالٹ سیٹنگز سے زیادہ تیزی سے جوابات ملیں گے۔ تاہم، Responses API میں کچھ بلٹ ان ٹولز (جیسے فائل سرچ اور امیج جنریشن) \"کم سے کم\" استدلال کی کوشش کو سپورٹ نہیں کرتے ہیں، یہی وجہ ہے کہ یہ ایجنٹس SDK ڈیفالٹ \"کم\" ہے۔\n",
    "\n",
    "غیر GPT-5 ماڈلز\n",
    "\n",
    "اگر آپ اپنی مرضی کے مطابق ماڈل_سیٹنگز کے بغیر ایک غیر-GPT-5 ماڈل کا نام پاس کرتے ہیں، تو SDK کسی بھی ماڈل کے ساتھ ہم آہنگ عمومی ModelSettings میں واپس آجاتا ہے۔\n",
    "\n",
    "غیر اوپن اے آئی ماڈلز\n",
    "\n",
    "آپ LiteLLM انٹیگریشن کے ذریعے زیادہ تر دیگر غیر OpenAI ماڈلز استعمال کر سکتے ہیں۔ سب سے پہلے، litellm انحصار گروپ انسٹال کریں:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84e46d6",
   "metadata": {},
   "source": [
    "Then, use any of the supported models with the litellm/ prefix:\n",
    "\n",
    "پھر، litellm/ prefix کے ساتھ کسی بھی معاون ماڈل کا استعمال کریں:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6c22ce",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "pip install \"openai-agents[litellm]\"\n",
    "\n",
    "claude_agent = Agent(model=\"litellm/anthropic/claude-3-5-sonnet-20240620\", ...)\n",
    "gemini_agent = Agent(model=\"litellm/gemini/gemini-2.5-flash-preview-04-17\", ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8381c4c9",
   "metadata": {},
   "source": [
    "Other ways to use non-OpenAI models\n",
    "\n",
    "You can integrate other LLM providers in 3 more ways (examples here):\n",
    "\n",
    "1. Set_default_openai_client is useful in cases where you want to globally use an instance of \n",
    "   AsyncOpenAI as the LLM client. This is for cases where the LLM provider has an OpenAI compatible API endpoint, and you can set the base_url and api_key. See a configurable example in examples/model_providers/custom_example_global.py.\n",
    "\n",
    "2. ModelProvider is at the Runner.run level. This lets you say \"use a custom model provider \n",
    "   for all agents in this run\". See a configurable example in examples/model_providers/custom_example_provider.py.\n",
    "\n",
    "3. Agent.model lets you specify the model on a specific Agent instance. This enables you to \n",
    "   mix and match different providers for different agents. See a configurable example in examples/model_providers/custom_example_agent.py. An easy way to use most available models is via the LiteLLM integration.\n",
    "\n",
    "4. In cases where you do not have an API key from platform.openai.com, we recommend disabling \n",
    "   tracing via set_tracing_disabled(), or setting up a different tracing processor.\n",
    "\n",
    "غیر اوپن اے آئی ماڈلز استعمال کرنے کے دوسرے طریقے\n",
    "\n",
    "آپ دوسرے LLM فراہم کنندگان کو مزید 3 طریقوں سے ضم کر سکتے ہیں (مثالیں یہاں):\n",
    "\n",
    "1. Set_default_openai_client ان صورتوں میں مفید ہے جہاں آپ عالمی سطح پر ایک مثال استعمال کرنا چاہتے ہیں \n",
    "AsyncOpenAI بطور LLM کلائنٹ۔ یہ ان صورتوں کے لیے ہے جہاں LLM فراہم کنندہ کے پاس OpenAI مطابقت پذیر API اینڈ پوائنٹ ہے، اور آپ base_url اور api_key سیٹ کر سکتے ہیں۔ مثالوں/model_providers/custom_example_global.py میں قابل ترتیب مثال دیکھیں۔\n",
    "\n",
    "2. ModelProvider Runner.run کی سطح پر ہے۔ یہ آپ کو یہ کہنے دیتا ہے کہ \"کسٹم ماڈل فراہم کنندہ استعمال کریں۔ \n",
    "اس رن میں تمام ایجنٹوں کے لیے۔ مثالوں/model_providers/custom_example_provider.py میں قابل ترتیب مثال دیکھیں۔\n",
    "\n",
    "3. Agent.model آپ کو ایک مخصوص ایجنٹ مثال پر ماڈل کی وضاحت کرنے دیتا ہے۔ یہ آپ کو قابل بناتا ہے۔ \n",
    "مختلف ایجنٹوں کے لیے مختلف فراہم کنندگان کو مکس اور میچ کریں۔ مثالوں/model_providers/custom_example_agent.py میں قابل ترتیب مثال دیکھیں۔ زیادہ تر دستیاب ماڈلز کو استعمال کرنے کا ایک آسان طریقہ LiteLLM انضمام کے ذریعے ہے۔\n",
    "\n",
    "4. ایسے معاملات میں جہاں آپ کے پاس platform.openai.com سے API کلید نہیں ہے، ہم اسے غیر فعال کرنے کی تجویز کرتے ہیں۔ \n",
    "set_tracing_disabled() کے ذریعے ٹریس کرنا، یا ایک مختلف ٹریسنگ پروسیسر ترتیب دینا۔"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1e2eee",
   "metadata": {},
   "source": [
    "Mixing and matching models\n",
    "\n",
    "Within a single workflow, you may want to use different models for each agent. For example, you could use a smaller, faster model for triage, while using a larger, more capable model for complex tasks. When configuring an Agent, you can select a specific model by either:\n",
    "\n",
    "1. Passing the name of a model.\n",
    "\n",
    "2. Passing any model name + a ModelProvider that can map that name to a Model instance.\n",
    "\n",
    "3. Directly providing a Model implementation.\n",
    "\n",
    "اختلاط اور ملاپ کے ماڈل\n",
    "\n",
    "ایک ہی ورک فلو کے اندر، آپ ہر ایجنٹ کے لیے مختلف ماڈل استعمال کرنا چاہیں گے۔ مثال کے طور پر، آپ ٹرائیج کے لیے ایک چھوٹا، تیز ماڈل استعمال کر سکتے ہیں، جبکہ پیچیدہ کاموں کے لیے ایک بڑا، زیادہ قابل ماڈل استعمال کر سکتے ہیں۔ ایجنٹ کی تشکیل کرتے وقت، آپ کسی مخصوص ماڈل کو منتخب کر سکتے ہیں:\n",
    "\n",
    "1. ماڈل کا نام پاس کرنا۔\n",
    "\n",
    "2. کسی بھی ماڈل کا نام + ایک ماڈل پرووائڈر پاس کرنا جو اس نام کو ماڈل مثال کے ساتھ نقشہ بنا سکے۔\n",
    "\n",
    "3. براہ راست ایک ماڈل کا نفاذ فراہم کرنا۔"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e8485b",
   "metadata": {},
   "source": [
    "While our SDK supports both the OpenAIResponsesModel and the OpenAIChatCompletionsModel shapes, we recommend using a single model shape for each workflow because the two shapes support a different set of features and tools. If your workflow requires mixing and matching model shapes, make sure that all the features you're using are available on both.\n",
    "\n",
    "اگرچہ ہمارا SDK OpenAIREsponsesModel اور OpenAIChatCompletionsModel دونوں شکلوں کو سپورٹ کرتا ہے، ہم ہر ورک فلو کے لیے ایک ماڈل کی شکل استعمال کرنے کی تجویز کرتے ہیں کیونکہ دونوں شکلیں خصوصیات اور ٹولز کے مختلف سیٹ کو سپورٹ کرتی ہیں۔ اگر آپ کے ورک فلو کو ماڈل کی شکلوں کو ملانے اور ملانے کی ضرورت ہے، تو یقینی بنائیں کہ آپ جو بھی خصوصیات استعمال کر رہے ہیں وہ دونوں پر دستیاب ہیں۔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7608c931",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from agents import Agent, Runner, AsyncOpenAI, OpenAIChatCompletionsModel\n",
    "import asyncio\n",
    "\n",
    "spanish_agent = Agent(\n",
    "    name=\"Spanish agent\",\n",
    "    instructions=\"You only speak Spanish.\",\n",
    "    model=\"gpt-5-mini\", \n",
    ")\n",
    "\n",
    "english_agent = Agent(\n",
    "    name=\"English agent\",\n",
    "    instructions=\"You only speak English\",\n",
    "    model=OpenAIChatCompletionsModel( \n",
    "        model=\"gpt-5-nano\",\n",
    "        openai_client=AsyncOpenAI()\n",
    "    ),\n",
    ")\n",
    "\n",
    "triage_agent = Agent(\n",
    "    name=\"Triage agent\",\n",
    "    instructions=\"Handoff to the appropriate agent based on the language of the request.\",\n",
    "    handoffs=[spanish_agent, english_agent],\n",
    "    model=\"gpt-5\",\n",
    ")\n",
    "\n",
    "async def main():\n",
    "    result = await Runner.run(triage_agent, input=\"Hola, ¿cómo estás?\")\n",
    "    print(result.final_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db39061",
   "metadata": {},
   "source": [
    "When you want to further configure the model used for an agent, you can pass ModelSettings, which provides optional model configuration parameters such as temperature.\n",
    "\n",
    "جب آپ کسی ایجنٹ کے لیے استعمال ہونے والے ماڈل کو مزید کنفیگر کرنا چاہتے ہیں، تو آپ ماڈل سیٹنگز پاس کر سکتے ہیں، جو اختیاری ماڈل کنفیگریشن پیرامیٹرز فراہم کرتی ہے جیسے درجہ حرارت۔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484cc9aa",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from agents import Agent, ModelSettings\n",
    "\n",
    "english_agent = Agent(\n",
    "    name=\"English agent\",\n",
    "    instructions=\"You only speak English\",\n",
    "    model=\"gpt-4.1\",\n",
    "    model_settings=ModelSettings(temperature=0.1),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327218e0",
   "metadata": {},
   "source": [
    "Also, when you use OpenAI's Responses API, there are a few other optional parameters (e.g., user, service_tier, and so on). If they are not available at the top level, you can use extra_args to pass them as well.\n",
    "\n",
    "اس کے علاوہ، جب آپ OpenAI کے Responses API کا استعمال کرتے ہیں، تو کچھ دوسرے اختیاری پیرامیٹرز ہوتے ہیں (جیسے صارف، service_tier، وغیرہ)۔ اگر وہ اوپر کی سطح پر دستیاب نہیں ہیں، تو آپ ان کو پاس کرنے کے لیے extra_args بھی استعمال کر سکتے ہیں۔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab727a4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from agents import Agent, ModelSettings\n",
    "\n",
    "english_agent = Agent(\n",
    "    name=\"English agent\",\n",
    "    instructions=\"You only speak English\",\n",
    "    model=\"gpt-4.1\",\n",
    "    model_settings=ModelSettings(\n",
    "        temperature=0.1,\n",
    "        extra_args={\"service_tier\": \"flex\", \"user\": \"user_12345\"},\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84abb9c",
   "metadata": {},
   "source": [
    "Common issues with using other LLM providers\n",
    "\n",
    "Tracing client error 401\n",
    "\n",
    "If you get errors related to tracing, this is because traces are uploaded to OpenAI servers, and you don't have an OpenAI API key. You have three options to resolve this:\n",
    "\n",
    "1. Disable tracing entirely: set_tracing_disabled(True).\n",
    "\n",
    "2. Set an OpenAI key for tracing: set_tracing_export_api_key(...). This API key will only be  \n",
    "   used for uploading traces, and must be from platform.openai.com.\n",
    "\n",
    "3. Use a non-OpenAI trace processor. See the tracing docs.\n",
    "\n",
    "دوسرے LLM فراہم کنندگان کے استعمال کے ساتھ عام مسائل\n",
    "\n",
    "کلائنٹ کی غلطی 401 کا سراغ لگانا\n",
    "\n",
    "اگر آپ کو ٹریسنگ سے متعلق غلطیاں ملتی ہیں، تو اس کی وجہ یہ ہے کہ نشانات OpenAI سرورز پر اپ لوڈ کیے جاتے ہیں، اور آپ کے پاس OpenAI API کلید نہیں ہے۔ اس کو حل کرنے کے لیے آپ کے پاس تین اختیارات ہیں:\n",
    "\n",
    "1. مکمل طور پر ٹریسنگ کو غیر فعال کریں: set_tracing_disabled(True)۔\n",
    "\n",
    "2. ٹریسنگ کے لیے ایک OpenAI کلید سیٹ کریں: set_tracing_export_api_key(...)۔ یہ API کلید صرف ہوگی۔ \n",
    "نشانات کو اپ لوڈ کرنے کے لیے استعمال کیا جاتا ہے، اور پلیٹ فارم.openai.com سے ہونا چاہیے۔\n",
    "\n",
    "3. ایک غیر اوپن اے آئی ٹریس پروسیسر استعمال کریں۔ ٹریسنگ دستاویزات دیکھیں۔\n",
    "\n",
    "Responses API support\n",
    "\n",
    "The SDK uses the Responses API by default, but most other LLM providers don't yet support it. You may see 404s or similar issues as a result. To resolve, you have two options:\n",
    "\n",
    "1. Call set_default_openai_api(\"chat_completions\"). This works if you are setting \n",
    "    OPENAI_API_KEY and OPENAI_BASE_URL via environment vars.\n",
    "\n",
    "2. Use OpenAIChatCompletionsModel. There are examples here.\n",
    "\n",
    "Structured outputs support\n",
    "\n",
    "Some model providers don't have support for structured outputs. This sometimes results in an error that looks something like this:\n",
    "\n",
    "جوابات API سپورٹ\n",
    "\n",
    "SDK بطور ڈیفالٹ Responses API استعمال کرتا ہے، لیکن زیادہ تر LLM فراہم کرنے والے ابھی تک اس کی حمایت نہیں کرتے ہیں۔ اس کے نتیجے میں آپ کو 404s یا اسی طرح کے مسائل نظر آ سکتے ہیں۔ حل کرنے کے لیے، آپ کے پاس دو اختیارات ہیں:\n",
    "\n",
    "1. کال کریں set_default_openai_api(\"chat_completions\")۔ اگر آپ ترتیب دے رہے ہیں تو یہ کام کرتا ہے۔ \n",
    "OPENAI_API_KEY اور OPENAI_BASE_URL بذریعہ ماحولیات۔\n",
    "\n",
    "2. OpenAIChatCompletionsModel استعمال کریں۔ یہاں مثالیں موجود ہیں۔\n",
    "\n",
    "ساختی آؤٹ پٹ سپورٹ\n",
    "\n",
    "کچھ ماڈل فراہم کنندگان کو سٹرکچرڈ آؤٹ پٹس کے لیے تعاون حاصل نہیں ہے۔ اس کے نتیجے میں بعض اوقات ایک غلطی ہوتی ہے جو کچھ اس طرح نظر آتی ہے:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468769ec",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "BadRequestError: Error code: 400 - {'error': {'message': \"'response_format.type' : value is not one of the allowed values ['text','json_object']\", 'type': 'invalid_request_error'}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4722b6e9",
   "metadata": {},
   "source": [
    "This is a shortcoming of some model providers - they support JSON outputs, but don't allow you to specify the json_schema to use for the output. We are working on a fix for this, but we suggest relying on providers that do have support for JSON schema output, because otherwise your app will often break because of malformed JSON.\n",
    "\n",
    "یہ کچھ ماڈل فراہم کنندگان کی کمی ہے - وہ JSON آؤٹ پٹ کو سپورٹ کرتے ہیں، لیکن آپ کو آؤٹ پٹ کے لیے استعمال کرنے کے لیے json_schema کی وضاحت کرنے کی اجازت نہیں دیتے ہیں۔ ہم اس کے حل پر کام کر رہے ہیں، لیکن ہم تجویز کرتے ہیں کہ ایسے فراہم کنندگان پر انحصار کریں جن کے پاس JSON اسکیما آؤٹ پٹ کے لیے سپورٹ ہے، کیونکہ بصورت دیگر آپ کی ایپ اکثر خراب JSON کی وجہ سے ٹوٹ جائے گی۔\n",
    "\n",
    "Mixing models across providers\n",
    "\n",
    "You need to be aware of feature differences between model providers, or you may run into errors. For example, OpenAI supports structured outputs, multimodal input, and hosted file search and web search, but many other providers don't support these features. Be aware of these limitations:\n",
    "\n",
    "* Don't send unsupported tools to providers that don't understand them\n",
    "\n",
    "* Filter out multimodal inputs before calling models that are text-only\n",
    "\n",
    "* Be aware that providers that don't support structured JSON outputs will occasionally \n",
    "  produce invalid JSON.\n",
    "\n",
    "فراہم کنندگان میں ماڈلز کو ملانا\n",
    "\n",
    "آپ کو ماڈل فراہم کنندگان کے درمیان خصوصیت کے فرق سے آگاہ ہونے کی ضرورت ہے، ورنہ آپ غلطیوں کا شکار ہو سکتے ہیں۔ مثال کے طور پر، OpenAI سٹرکچرڈ آؤٹ پٹ، ملٹی موڈل ان پٹ، اور ہوسٹڈ فائل سرچ اور ویب سرچ کو سپورٹ کرتا ہے، لیکن بہت سے دوسرے فراہم کنندگان ان خصوصیات کو سپورٹ نہیں کرتے ہیں۔ ان حدود سے آگاہ رہیں:\n",
    "\n",
    "* ایسے فراہم کنندگان کو غیر تعاون یافتہ ٹولز نہ بھیجیں جو انہیں نہیں سمجھتے ہیں۔\n",
    "\n",
    "* صرف ٹیکسٹ والے ماڈلز کو کال کرنے سے پہلے ملٹی موڈل ان پٹس کو فلٹر کریں۔\n",
    "\n",
    "* آگاہ رہیں کہ وہ فراہم کنندگان جو ساختی JSON آؤٹ پٹس کو سپورٹ نہیں کرتے ہیں کبھی کبھار ایسا کرتے ہیں۔ \n",
    "غلط JSON تیار کریں۔"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7df7e08",
   "metadata": {},
   "source": [
    "Using Any Model Via Lite LLM / لائٹ ایل ایل ایم کے ذریعے کوئی بھی ماڈل استعمال کرنا\n",
    "\n",
    "The LiteLLM integration is in beta. You may run into issues with some model providers, especially smaller ones. Please report any issues via Github issues and we'll fix quickly.\n",
    "\n",
    "LiteLLM انضمام بیٹا میں ہے۔ آپ کو کچھ ماڈل فراہم کنندگان کے ساتھ مسائل کا سامنا کرنا پڑ سکتا ہے، خاص طور پر چھوٹے۔ براہ کرم گیتھب کے مسائل کے ذریعے کسی بھی مسئلے کی اطلاع دیں اور ہم اسے جلد ٹھیک کر دیں گے۔\n",
    "\n",
    "LiteLLM is a library that allows you to use 100+ models via a single interface. We've added a LiteLLM integration to allow you to use any AI model in the Agents SDK.\n",
    "\n",
    "LiteLLM ایک لائبریری ہے جو آپ کو ایک انٹرفیس کے ذریعے 100+ ماڈل استعمال کرنے کی اجازت دیتی ہے۔ ہم نے ایک LiteLLM انضمام شامل کیا ہے تاکہ آپ کو ایجنٹس SDK میں کسی بھی AI ماڈل کو استعمال کرنے کی اجازت دی جائے۔\n",
    "\n",
    "Setup\n",
    "\n",
    "You'll need to ensure litellm is available. You can do this by installing the optional litellm dependency group:\n",
    "\n",
    "سیٹ اپ\n",
    "\n",
    "آپ کو یہ یقینی بنانا ہوگا کہ litellm دستیاب ہے۔ آپ اختیاری litellm انحصار گروپ کو انسٹال کرکے ایسا کرسکتے ہیں:\n",
    "\n",
    "pip install \"openai-agents[litellm]\"\n",
    "\n",
    "Once Done, You Can Use Lite LLM Model In Any Agent. / ایک بار مکمل ہونے کے بعد، آپ کسی بھی ایجنٹ میں LitellmModel استعمال کر سکتے ہیں۔\n",
    "\n",
    "Example\n",
    "\n",
    "This is a fully working example. When you run it, you'll be prompted for a model name and API key. For example, you could enter:\n",
    "\n",
    "* Openai/gpt-4.1 for the model, and your OpenAI API key\n",
    "\n",
    "*. Anthropic/claude-3-5-sonnet-20240620 for the model, and your Anthropic API key\n",
    "\n",
    "* etc\n",
    "\n",
    "For a full list of models supported in LiteLLM, see the litellm providers docs.\n",
    "\n",
    "مثال\n",
    "\n",
    "یہ مکمل طور پر کام کرنے والی مثال ہے۔ جب آپ اسے چلاتے ہیں، تو آپ کو ایک ماڈل کا نام اور API کلید کے لیے کہا جائے گا۔ مثال کے طور پر، آپ درج کر سکتے ہیں:\n",
    "\n",
    "* ماڈل کے لیے Openai/gpt-4.1، اور آپ کی OpenAI API کلید\n",
    "\n",
    "* ماڈل کے لیے Anthropic/claude-3-5-sonnet-20240620، اور آپ کی Anthropic API کلید\n",
    "\n",
    "*وغیرہ\n",
    "\n",
    "LiteLLM میں تعاون یافتہ ماڈلز کی مکمل فہرست کے لیے، litellm فراہم کنندگان کی دستاویزات دیکھیں۔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c0ecca",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import asyncio\n",
    "\n",
    "from agents import Agent, Runner, function_tool, set_tracing_disabled\n",
    "from agents.extensions.models.litellm_model import LitellmModel\n",
    "\n",
    "@function_tool\n",
    "def get_weather(city: str):\n",
    "    print(f\"[debug] getting weather for {city}\")\n",
    "    return f\"The weather in {city} is sunny.\"\n",
    "\n",
    "\n",
    "async def main(model: str, api_key: str):\n",
    "    agent = Agent(\n",
    "        name=\"Assistant\",\n",
    "        instructions=\"You only respond in haikus.\",\n",
    "        model=LitellmModel(model=model, api_key=api_key),\n",
    "        tools=[get_weather],\n",
    "    )\n",
    "\n",
    "    result = await Runner.run(agent, \"What's the weather in Tokyo?\")\n",
    "    print(result.final_output)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # First try to get model/api key from args\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--model\", type=str, required=False)\n",
    "    parser.add_argument(\"--api-key\", type=str, required=False)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    model = args.model\n",
    "    if not model:\n",
    "        model = input(\"Enter a model name for Litellm: \")\n",
    "\n",
    "    api_key = args.api_key\n",
    "    if not api_key:\n",
    "        api_key = input(\"Enter an API key for Litellm: \")\n",
    "\n",
    "    asyncio.run(main(model, api_key))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f826c468",
   "metadata": {},
   "source": [
    "Tracking usage data\n",
    "\n",
    "If you want LiteLLM responses to populate the Agents SDK usage metrics, pass ModelSettings(include_usage=True) when creating your agent.\n",
    "\n",
    "استعمال کے ڈیٹا کو ٹریک کرنا\n",
    "\n",
    "اگر آپ چاہتے ہیں کہ LiteLLM جوابات ایجنٹوں کے SDK کے استعمال کے میٹرکس کو آباد کریں تو اپنا ایجنٹ بناتے وقت ModelSettings(include_usage=True) پاس کریں۔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924f0ce9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from agents import Agent, ModelSettings\n",
    "from agents.extensions.models.litellm_model import LitellmModel\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"Assistant\",\n",
    "    model=LitellmModel(model=\"your/model\", api_key=\"...\"),\n",
    "    model_settings=ModelSettings(include_usage=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86949819",
   "metadata": {},
   "source": [
    "With include_usage=True, LiteLLM requests report token and request counts through result.context_wrapper.usage just like the built-in OpenAI models.\n",
    "\n",
    "Include_usage=True کے ساتھ، LiteLLM بلٹ ان OpenAI ماڈلز کی طرح result.context_wrapper.usage کے ذریعے رپورٹ ٹوکن اور گنتی کی درخواست کرتا ہے۔"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
