{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76eddd24",
   "metadata": {},
   "source": [
    "ModelSettings\n",
    "\n",
    "Settings to use when calling an LLM.\n",
    "\n",
    "This class holds optional model configuration parameters (e.g. temperature, top_p, penalties, truncation, etc.).\n",
    "\n",
    "Not all models/providers support all of these parameters, so please check the API documentation for the specific model and provider you are using.\n",
    "\n",
    "ماڈل سیٹنگز\n",
    "\n",
    "ایل ایل ایم کو کال کرتے وقت استعمال کرنے کی ترتیبات۔\n",
    "\n",
    "یہ کلاس اختیاری ماڈل کنفیگریشن پیرامیٹرز رکھتی ہے (جیسے درجہ حرارت، top_p، جرمانے، تراشنا وغیرہ)۔\n",
    "\n",
    "تمام ماڈلز/فراہم کنندگان ان تمام پیرامیٹرز کو سپورٹ نہیں کرتے ہیں، اس لیے براہ کرم اس مخصوص ماڈل اور فراہم کنندہ کے لیے API دستاویزات کو چیک کریں جسے آپ استعمال کر رہے ہیں۔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855f98fc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelSettings:\n",
    "    \"\"\"Settings to use when calling an LLM.\n",
    "\n",
    "    This class holds optional model configuration parameters (e.g. temperature,\n",
    "    top_p, penalties, truncation, etc.).\n",
    "\n",
    "    Not all models/providers support all of these parameters, so please check the API documentation\n",
    "    for the specific model and provider you are using.\n",
    "    \"\"\"\n",
    "\n",
    "    temperature: float | None = None\n",
    "    \"\"\"The temperature to use when calling the model.\"\"\"\n",
    "\n",
    "    top_p: float | None = None\n",
    "    \"\"\"The top_p to use when calling the model.\"\"\"\n",
    "\n",
    "    frequency_penalty: float | None = None\n",
    "    \"\"\"The frequency penalty to use when calling the model.\"\"\"\n",
    "\n",
    "    presence_penalty: float | None = None\n",
    "    \"\"\"The presence penalty to use when calling the model.\"\"\"\n",
    "\n",
    "    tool_choice: ToolChoice | None = None\n",
    "    \"\"\"The tool choice to use when calling the model.\"\"\"\n",
    "\n",
    "    parallel_tool_calls: bool | None = None\n",
    "    \"\"\"Controls whether the model can make multiple parallel tool calls in a single turn.\n",
    "    If not provided (i.e., set to None), this behavior defers to the underlying\n",
    "    model provider's default. For most current providers (e.g., OpenAI), this typically\n",
    "    means parallel tool calls are enabled (True).\n",
    "    Set to True to explicitly enable parallel tool calls, or False to restrict the\n",
    "    model to at most one tool call per turn.\n",
    "    \"\"\"\n",
    "\n",
    "    truncation: Literal[\"auto\", \"disabled\"] | None = None\n",
    "    \"\"\"The truncation strategy to use when calling the model.\n",
    "    See [Responses API documentation](https://platform.openai.com/docs/api-reference/responses/create#responses_create-truncation)\n",
    "    for more details.\n",
    "    \"\"\"\n",
    "\n",
    "    max_tokens: int | None = None\n",
    "    \"\"\"The maximum number of output tokens to generate.\"\"\"\n",
    "\n",
    "    reasoning: Reasoning | None = None\n",
    "    \"\"\"Configuration options for\n",
    "    [reasoning models](https://platform.openai.com/docs/guides/reasoning).\n",
    "    \"\"\"\n",
    "\n",
    "    verbosity: Literal[\"low\", \"medium\", \"high\"] | None = None\n",
    "    \"\"\"Constrains the verbosity of the model's response.\n",
    "    \"\"\"\n",
    "\n",
    "    metadata: dict[str, str] | None = None\n",
    "    \"\"\"Metadata to include with the model response call.\"\"\"\n",
    "\n",
    "    store: bool | None = None\n",
    "    \"\"\"Whether to store the generated model response for later retrieval.\n",
    "    For Responses API: automatically enabled when not specified.\n",
    "    For Chat Completions API: disabled when not specified.\"\"\"\n",
    "\n",
    "    include_usage: bool | None = None\n",
    "    \"\"\"Whether to include usage chunk.\n",
    "    Only available for Chat Completions API.\"\"\"\n",
    "\n",
    "    # TODO: revisit ResponseIncludable | str if ResponseIncludable covers more cases\n",
    "    # We've added str to support missing ones like\n",
    "    # \"web_search_call.action.sources\" etc.\n",
    "    response_include: list[ResponseIncludable | str] | None = None\n",
    "    \"\"\"Additional output data to include in the model response.\n",
    "    [include parameter](https://platform.openai.com/docs/api-reference/responses/create#responses-create-include)\"\"\"\n",
    "\n",
    "    top_logprobs: int | None = None\n",
    "    \"\"\"Number of top tokens to return logprobs for. Setting this will\n",
    "    automatically include ``\"message.output_text.logprobs\"`` in the response.\"\"\"\n",
    "\n",
    "    extra_query: Query | None = None\n",
    "    \"\"\"Additional query fields to provide with the request.\n",
    "    Defaults to None if not provided.\"\"\"\n",
    "\n",
    "    extra_body: Body | None = None\n",
    "    \"\"\"Additional body fields to provide with the request.\n",
    "    Defaults to None if not provided.\"\"\"\n",
    "\n",
    "    extra_headers: Headers | None = None\n",
    "    \"\"\"Additional headers to provide with the request.\n",
    "    Defaults to None if not provided.\"\"\"\n",
    "\n",
    "    extra_args: dict[str, Any] | None = None\n",
    "    \"\"\"Arbitrary keyword arguments to pass to the model API call.\n",
    "    These will be passed directly to the underlying model provider's API.\n",
    "    Use with caution as not all models support all parameters.\"\"\"\n",
    "\n",
    "    def resolve(self, override: ModelSettings | None) -> ModelSettings:\n",
    "        \"\"\"Produce a new ModelSettings by overlaying any non-None values from the\n",
    "        override on top of this instance.\"\"\"\n",
    "        if override is None:\n",
    "            return self\n",
    "\n",
    "        changes = {\n",
    "            field.name: getattr(override, field.name)\n",
    "            for field in fields(self)\n",
    "            if getattr(override, field.name) is not None\n",
    "        }\n",
    "\n",
    "        # Handle extra_args merging specially - merge dictionaries instead of replacing\n",
    "        if self.extra_args is not None or override.extra_args is not None:\n",
    "            merged_args = {}\n",
    "            if self.extra_args:\n",
    "                merged_args.update(self.extra_args)\n",
    "            if override.extra_args:\n",
    "                merged_args.update(override.extra_args)\n",
    "            changes[\"extra_args\"] = merged_args if merged_args else None\n",
    "\n",
    "        return replace(self, **changes)\n",
    "\n",
    "    def to_json_dict(self) -> dict[str, Any]:\n",
    "        dataclass_dict = dataclasses.asdict(self)\n",
    "\n",
    "        json_dict: dict[str, Any] = {}\n",
    "\n",
    "        for field_name, value in dataclass_dict.items():\n",
    "            if isinstance(value, BaseModel):\n",
    "                json_dict[field_name] = value.model_dump(mode=\"json\")\n",
    "            else:\n",
    "                json_dict[field_name] = value\n",
    "\n",
    "        return json_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe03c47",
   "metadata": {},
   "source": [
    "Temperature Class-Attribute Instance-Attribute\n",
    "\n",
    "درجہ حرارت کی کلاس-انتساب مثال-خصوصیت\n",
    "\n",
    "temperature: float | None = None\n",
    "\n",
    "The Temperature To Use When Calling The Model.\n",
    "\n",
    "ماڈل کو کال کرتے وقت استعمال کرنے کا درجہ حرارت۔\n",
    "\n",
    "Top_P Class-Attribute Instance-Attribute\n",
    "\n",
    "سرفہرست _ پی کلاس - انتساب مثال- وصف\n",
    "\n",
    "top_p: float | None = None\n",
    "\n",
    "The Top_P To Use When Calling The Model.\n",
    "\n",
    "ماڈل کو کال کرتے وقت استعمال کرنے کے لیے ٹاپ_پی۔\n",
    "\n",
    "Frequency_penalty Class-Attribute Instance-Attribute\n",
    "\n",
    "تعدد_جرمانہ کلاس-انتساب مثال- خصوصیت\n",
    "\n",
    "frequency_penalty: float | None = None\n",
    "\n",
    "The Frequency Penalty To Use When Calling The Model.\n",
    "\n",
    "ماڈل کو کال کرتے وقت استعمال کرنے کے لیے تعدد جرمانہ۔\n",
    "\n",
    "Presence_Penalty Class-Attribute Instance-Attribute\n",
    "\n",
    "موجودگی_پینلٹی کلاس-انتساب مثال-وصف\n",
    "\n",
    "presence_penalty: float | None = None\n",
    "\n",
    "The Presence Penalty To Use when Calling The Model.\n",
    "\n",
    "ماڈل کو کال کرتے وقت استعمال کرنے کے لیے موجودگی کا جرمانہ۔\n",
    "\n",
    "Tool_Choice Class-Attribute Instance-Attribute\n",
    "\n",
    "ٹول_چوائس کلاس - انتساب مثال - خصوصیت\n",
    "\n",
    "tool_choice: ToolChoice | None = None\n",
    "\n",
    "\n",
    "The Tool choice To Use When Calling The Model.\n",
    "\n",
    "ماڈل کو کال کرتے وقت استعمال کرنے کے لیے ٹول کا انتخاب۔\n",
    "\n",
    "Parallel_Tool_Calls Class-Attribute Instance-Attribute\n",
    "\n",
    "متوازی_ ٹول _ کلاس کو کال کرتا ہے۔\n",
    "\n",
    "parallel_tool_calls: bool | None = None\n",
    "\n",
    "Controls whether the model can make multiple parallel tool calls in a single turn. If not provided (i.e., set to None), this behavior defers to the underlying model provider's default. For most current providers (e.g., OpenAI), this typically means parallel tool calls are enabled (True). Set to True to explicitly enable parallel tool calls, or False to restrict the model to at most one tool call per turn.\n",
    "\n",
    "کنٹرول کرتا ہے کہ آیا ماڈل ایک ہی موڑ میں متعدد متوازی ٹول کالز کر سکتا ہے۔ اگر فراہم نہیں کیا جاتا ہے (یعنی، کوئی نہیں پر سیٹ کیا جاتا ہے)، تو یہ رویہ بنیادی ماڈل فراہم کنندہ کے ڈیفالٹ سے موخر ہوجاتا ہے۔ زیادہ تر موجودہ فراہم کنندگان (جیسے، OpenAI) کے لیے، اس کا عام طور پر مطلب ہے کہ متوازی ٹول کالز فعال ہیں (True)۔ متوازی ٹول کالز کو واضح طور پر فعال کرنے کے لیے سچ پر سیٹ کریں، یا فی موڑ زیادہ سے زیادہ ایک ٹول کال تک ماڈل کو محدود کرنے کے لیے False پر سیٹ کریں۔\n",
    "\n",
    "Truncation Class-Attribute Instance-Attribute\n",
    "\n",
    "ٹرنکیشن کلاس-انتساب مثال-خصوصیت\n",
    "\n",
    "truncation: Literal['auto', 'disabled'] | None = None\n",
    "\n",
    "The truncation strategy to use when calling the model. See Responses API documentation for more details.\n",
    "\n",
    "ماڈل کو کال کرتے وقت استعمال کرنے کے لیے تراشنے کی حکمت عملی۔ مزید تفصیلات کے لیے جوابات API دستاویزات دیکھیں۔\n",
    "\n",
    "Max_Tokens Class-aAtribute Instance-Attribute\n",
    "\n",
    "زیادہ سے زیادہ _ ٹوکن کلاس - انتساب مثال - وصف\n",
    "\n",
    "max_tokens: int | None = None\n",
    "\n",
    "The Maximum Number Of OutPut Tokens To Generate.\n",
    "\n",
    "جنریٹ کرنے کے لیے آؤٹ پٹ ٹوکنز کی زیادہ سے زیادہ تعداد۔\n",
    "\n",
    "Reasoning Class-Attribute Instance-Attribute\n",
    "\n",
    "ریزننگ کلاس - انتساب مثال - خاصیت\n",
    "\n",
    "reasoning: Reasoning | None = None\n",
    "\n",
    "Configuration Options For Reasoning Models.\n",
    "\n",
    "ریزننگ ماڈلز کے لیے ترتیب کے اختیارات۔\n",
    "\n",
    "Verbosity Class-Attribute Instance-Attribute\n",
    "\n",
    "وربوسٹی کلاس - انتساب مثال - وصف\n",
    "\n",
    "verbosity: Literal['low', 'medium', 'high'] | None = None\n",
    "\n",
    "Constrains The Verbosity Of The Model's Response.\n",
    "\n",
    "ماڈل کے جواب کی لفظی صلاحیت کو محدود کرتا ہے۔\n",
    "\n",
    "Metadata Class-Attribute Instance-Attribute\n",
    "\n",
    "میٹا ڈیٹا کلاس- انتساب مثال- وصف\n",
    "\n",
    "metadata: dict[str, str] | None = None\n",
    "\n",
    "Metadata To Include With The Model Response Call.\n",
    "\n",
    "ماڈل ریسپانس کال کے ساتھ شامل کرنے کے لیے میٹا ڈیٹا۔\n",
    "\n",
    "Store Class-Attribute Instance-Attribute\n",
    "\n",
    "اسٹور کلاس- انتساب مثال - انتساب\n",
    "\n",
    "store: bool | None = None\n",
    "\n",
    "Whether to store the generated model response for later retrieval. For Responses API: automatically enabled when not specified. For Chat Completions API: disabled when not specified.\n",
    "\n",
    "آیا بعد میں بازیافت کے لیے تیار کردہ ماڈل کے جواب کو اسٹور کرنا ہے۔ جوابات API کے لیے: متعین نہ ہونے پر خود بخود فعال ہوجاتا ہے۔ چیٹ کی تکمیل کے لیے A P I : مخصوص نہ ہونے پر غیر فعال۔\n",
    "\n",
    "Include_Usage Class-Attribute Instance-Attribute\n",
    "\n",
    "شامل_ استعمال کی کلاس- وصف مثال- وصف\n",
    "\n",
    "include_usage: bool | None = None\n",
    "\n",
    "Whether to include usage chunk. Only available for Chat Completions API.\n",
    "\n",
    "آیا استعمال کا حصہ شامل کرنا ہے۔ صرف چیٹ تکمیل API کے لیے دستیاب ہے۔\n",
    "\n",
    "Response_Include Class-Attribute Instance-Attribute\n",
    "\n",
    "جواب_ کلاس- وصف مثال- خصوصیت شامل کریں۔\n",
    "\n",
    "response_include: list[ResponseIncludable | str] | None = (\n",
    "    None\n",
    ")\n",
    "\n",
    "Additional output data to include in the model response. include parameter\n",
    "\n",
    "ماڈل کے جواب میں شامل کرنے کے لیے اضافی آؤٹ پٹ ڈیٹا۔ پیرامیٹر شامل کریں\n",
    "\n",
    "Top_Logprobs Class-Attribute Instance-Attribute\n",
    "\n",
    "ٹاپ _ لاگ پروبس کلاس - انتساب مثال - انتساب\n",
    "\n",
    "top_logprobs: int | None = None\n",
    "\n",
    "Number of top tokens to return logprobs for. Setting this will automatically include \"message.output_text.logprobs\" in the response.\n",
    "\n",
    "لاگ پروبس کو واپس کرنے کے لیے ٹاپ ٹوکنز کی تعداد۔ اسے ترتیب دینے سے جواب میں خود بخود \"پیغام۔ آؤٹ پٹ _ ٹیکسٹ۔ لاگ پروبس\" شامل ہو جائے گا۔\n",
    "\n",
    "Extra_Query Class-Attribute Instance-Attribute\n",
    "\n",
    "اضافی _ سوال کی کلاس - خصوصیت مثال - خصوصیت\n",
    "\n",
    "extra_query: Query | None = None\n",
    "\n",
    "Additional query fields to provide with the request. Defaults to None if not provided.\n",
    "\n",
    "درخواست کے ساتھ فراہم کرنے کے لیے اضافی سوال کے فیلڈز۔ اگر فراہم نہ کیا گیا ہو تو ڈیفالٹس کوئی نہیں۔\n",
    "\n",
    "Extra_Body Class-Attribute Instance-Attribute\n",
    "\n",
    "اضافی _ باڈی کلاس- وصف مثال- وصف\n",
    "\n",
    "extra_body: Body | None = None\n",
    "\n",
    "Additional body fields to provide with the request. Defaults to None if not provided.\n",
    "\n",
    "درخواست کے ساتھ فراہم کرنے کے لیے اضافی باڈی فیلڈز۔ اگر فراہم نہ کیا گیا ہو تو ڈیفالٹس کوئی نہیں۔\n",
    "\n",
    "Extra_Headers Class-Attribute Instance-Attribute\n",
    "\n",
    "اضافی _ ہیڈر کلاس - انتساب کی مثال - خصوصیت\n",
    "\n",
    "extra_headers: Headers | None = None\n",
    "\n",
    "Additional headers to provide with the request. Defaults to None if not provided.\n",
    "\n",
    "درخواست کے ساتھ فراہم کرنے کے لیے اضافی ہیڈرز۔ اگر فراہم نہ کیا گیا ہو تو ڈیفالٹس کوئی نہیں۔\n",
    "\n",
    "Extra_Args Class-Attribute Instance-Attribute\n",
    "\n",
    "اضافی _ A r g s کلاس - انتساب مثال - وصف\n",
    "\n",
    "extra_args: dict[str, Any] | None = None\n",
    "\n",
    "Arbitrary keyword arguments to pass to the model API call. These will be passed directly to the underlying model provider's API. Use with caution as not all models support all parameters.\n",
    "\n",
    "ماڈل API کال کو منتقل کرنے کے لیے صوابدیدی مطلوبہ الفاظ کے دلائل۔ یہ براہ راست بنیادی ماڈل فراہم کنندہ کے API کو بھیجے جائیں گے۔ احتیاط کے ساتھ استعمال کریں کیونکہ تمام ماڈل تمام پیرامیٹرز کی حمایت نہیں کرتے ہیں۔\n",
    "\n",
    "Resolve / حل کریں۔\n",
    "\n",
    "resolve(override: ModelSettings | None) -> ModelSettings\n",
    "\n",
    "Produce a new ModelSettings by overlaying any non-None values from the override on top of this instance.\n",
    "\n",
    "اس مثال کے اوپر اوور رائڈ سے کسی بھی غیر کوئی بھی قدر کو اوورلے کرکے ایک نئی ماڈل سیٹنگ تیار کریں۔"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
